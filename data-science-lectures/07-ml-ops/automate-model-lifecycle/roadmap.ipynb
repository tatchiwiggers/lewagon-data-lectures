{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1b6482b-95b4-4ecb-a71b-bcd51cb71625",
   "metadata": {},
   "source": [
    "# ML FLow x Prefect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ad91c4-a20f-4d4e-95d2-632b7aa86e86",
   "metadata": {},
   "source": [
    "Prefect and MLflow are two different tools with different use cases, but there can be some overlap in the context of data science and machine learning workflows.\n",
    "\n",
    "1. **Prefect** is a workflow management system built in Python. It's designed to help data scientists and data engineers build, test, and run data workflows. Prefect workflows consist of tasks that form a directed acyclic graph (DAG), and Prefect takes care of running those tasks in the correct order, handling failures, and logging what happened. It can be used to orchestrate complex machine learning pipelines but is not specifically designed for machine learning.\n",
    "\n",
    "2. **MLflow**, on the other hand, is an open-source platform specifically designed for managing the end-to-end machine learning lifecycle. It includes tools for tracking experiments, packaging code into reproducible runs, managing and deploying models. Here's a breakdown of MLflow's main components:\n",
    "    - **MLflow Tracking**: for logging and querying experiments, including code, data, config, and results.\n",
    "    - **MLflow Projects**: for packaging ML code in a reusable, reproducible form to share with other data scientists or transfer to production.\n",
    "    - **MLflow Models**: for managing and deploying models from different ML libraries to a variety of model serving and inference platforms.\n",
    "    - **MLflow Model Registry**: for collaborative model lifecycle management.\n",
    "\n",
    "In summary, Prefect is more of a general-purpose data workflow tool that's useful in a variety of contexts, including but not limited to machine learning. MLflow, on the other hand, is specifically built to manage various stages in the machine learning lifecycle, including experimentation, reproducibility, and deployment. They can even be used together: Prefect can be used to orchestrate an MLflow-based machine learning workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1da75d5-802f-4ab3-acc9-67306741df8b",
   "metadata": {},
   "source": [
    "# ML Flow Roadmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a845384-6291-4727-ae7c-1b52e44e4782",
   "metadata": {},
   "source": [
    "With ML flow there is a few things we need to do. First we need to set a tracking URI which tells ML FLow which server we want to store everything on. We will be using Le Wagon's server, but if you had your own server, this is where you would pass it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7e8535d-5898-47f9-9a13-4debb82552d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"https://mlflow.lewagon.ai\")\n",
    "mlflow.set_experiment(experiment_name=\"wagoncab taxifare\") # choose an experiment name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c9794b-e44a-4a82-a50f-10599fb3b6aa",
   "metadata": {},
   "source": [
    "Then start a run. Note that this is a reproduceable piece of code. We do not have to re-write it for every single run. All we have to do is reuse it by passing a `python decorator` in our packages - we'll see how in a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c201a93-5ffd-4ec0-989b-7ad3d233d345",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "\n",
    "    params = dict(batch_size=256, row_count=100_000)\n",
    "    metrics = dict(rmse=0.456)\n",
    "\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_metrics(metrics)\n",
    "\n",
    "    mlflow.tensorflow.log_model(model=model,\n",
    "                                artifact_path=\"model\",\n",
    "                                registered_model_name=\"taxifare_model\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f525f26b-e303-4142-ad9a-b462db441c70",
   "metadata": {},
   "source": [
    "Finally, the more critical part is getting the model back - once we do that we load the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b02902-fb28-47fd-953c-15c911727f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD MODEL\n",
    "import mlflow\n",
    "\n",
    "# we have to do this so mlflow knows which server its working with\n",
    "mlflow.set_tracking_uri(\"https://mlflow.lewagon.ai\")\n",
    "\n",
    "# on tha server there will be a path to that model\n",
    "model_uri = \"models:/taxifare_model/Production\"\n",
    "\n",
    "# finally we load the model\n",
    "model = mlflow.tensorflow.load_model(model_uri=source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685acb85-cc8e-4d55-9ae8-8f0abbfa1881",
   "metadata": {},
   "source": [
    "**This is nice, but where does this go in our package?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7408b7-66e5-4576-97bd-58333c79b8c4",
   "metadata": {},
   "source": [
    "First, we will explore the `mlflow_run` decorator in `ml_logic/registry.py`. Here we will:\n",
    "- add parameters\n",
    "- add metrics\n",
    "- store model\n",
    "- make a prediciton with the stored model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5db857-b089-440e-ac8a-3c4c365a3bba",
   "metadata": {},
   "source": [
    "Let's check the `mlflow_run` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb0ec1b-e6f3-49ba-81ca-bb5ec60561ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# registry.py\n",
    "\n",
    "def mlflow_run(func):\n",
    "    \"\"\"\n",
    "    Generic function to log params and results to MLflow along with TensorFlow auto-logging\n",
    "\n",
    "    Args:\n",
    "        - func (function): Function you want to run within the MLflow run\n",
    "        - params (dict, optional): Params to add to the run in MLflow. Defaults to None.\n",
    "        - context (str, optional): Param describing the context of the run. Defaults to \"Train\".\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        mlflow.end_run()\n",
    "        mlflow.set_tracking_uri(MLFLOW_TRACKING_URI) # variable set in our .env file\n",
    "        mlflow.set_experiment(experiment_name=MLFLOW_EXPERIMENT) # variable set in our .env file\n",
    "        # starts the run\n",
    "        with mlflow.start_run():\n",
    "            # automatically does the parameters and metrics so we don't have \n",
    "            # to specify every single parameter and metrics that we need\n",
    "            # autolog will save absolutely everything it can find about this training\n",
    "            mlflow.tensorflow.autolog()\n",
    "            # here we run the original function that was decorated\n",
    "            results = func(*args, **kwargs)\n",
    "\n",
    "        print(\"✅ mlflow_run auto-log done\")\n",
    "        # here we return the results of that function\n",
    "        return results\n",
    "    # here we return the wrapper function\n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa069a0-a0cb-4c09-b79d-4ecd614945cc",
   "metadata": {},
   "source": [
    "Now we need to update our `.env` file so that it has the correct environment variables for mlflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523536c6-ac72-491f-8d76-91f13a78e60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TARGET=mlflow\n",
    "\n",
    "# Model Lifecycle\n",
    "MLFLOW_TRACKING_URI=https://mlflow.lewagon.ai # this is le wagon's server.\n",
    "MLFLOW_EXPERIMENT=taxifare_experiment_<experiment_name>\n",
    "MLFLOW_MODEL_NAME=taxifare_<experiment_name>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df60d262-3d99-41bf-b922-4494a643ea0f",
   "metadata": {},
   "source": [
    "Now we need to make sure the decorator is added to the training. So we'll go to `interface/main.py` and loot at the function `def train`. You will notice that is an `@mlflow_run` decorator right above the function. This means that the decorator will run the `mlflow_run` funtion before it executes the train function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4f7e7c-2582-4e73-b564-d46ec04603d1",
   "metadata": {},
   "source": [
    "Let's test and see what happens!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837f3af5-c243-4619-b633-6fa1a13bc908",
   "metadata": {},
   "source": [
    "1. run `make run_train`\n",
    "2. head over to the website and check your experiment https://mlflow.lewagon.ai/\n",
    "3. search your experiment by experiment name\n",
    "4. and voilà!\n",
    "5. There you can see the details of your run - be curious dig around. Click on the `minutes`column to take a look at your metrics tab. When youclick on a specific metric, it will populate the chart for that metric! Also notice the parameters tab? How many are there? Which are they?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8236970f-17d1-4539-92f2-67b9c0c77c03",
   "metadata": {},
   "source": [
    "Now we want to store our model. At the moment, at the end of the `main.py` file we have the function the `df train` funtion that contains a `save_model` function call. `ctrl + right-click` the `save_model` function to be taken to where the function is at. It's inside `registry.py`. Below the `if MODELT_TARGET == 'gcs'` line, we'll add:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4be6d4-970c-42c9-8eeb-bf4c4d98ef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_TARGET ==  'mlflow':\n",
    "        mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "        mlflow.tensorflow.log_model(model,\n",
    "                                    registered_model_name=MLFLOW_MODEL_NAME,\n",
    "                                    artifact_path='models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3140baa-69e8-4d18-a7d5-99b9552bcd39",
   "metadata": {},
   "source": [
    "Now, when we run `make run_train`, you will be able to actually access a version of your model and we also able to call it back to python if you want to! so rerun `make run_train` and go back to the experiment on the website. Now, when you click on the `models` column, you have a fully usable model that can be pulled back into python to make a prediciton with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e803f47b-dec0-41cd-8239-8277ae330b03",
   "metadata": {},
   "source": [
    "Here you can also set the stages of you model:\n",
    "- staging\n",
    "- production\n",
    "- archived"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da210cd-abce-4f31-818b-537413bc9b66",
   "metadata": {},
   "source": [
    "Let's load our model m=back in for prediction. Go to `registry.py` and find the `def load_model` function and write some code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e82f8df-f7b3-4c0b-83ce-9b08e5f17e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[...]\n",
    "\n",
    "elif MODEL_TARGET == \"mlflow\":\n",
    "        print(Fore.BLUE + f\"\\nLoad [{stage}] model from MLflow...\" + Style.RESET_ALL)\n",
    "\n",
    "        # Load model from MLflow\n",
    "        model = None\n",
    "        # set the tracking URI as always\n",
    "        mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "        # create a client to interact with the server and pull down the path to our latest model\n",
    "        client = mlflow.tracking.MlflowClient()\n",
    "        # gets the lates versions of our model in the current stage (in our case Production)\n",
    "        version = client.get_latest_versions(MLFLOW_MODEL_NAME, stages=[stage])\n",
    "        # version is a list containin rhe ModelVersion Object. Here we access the model\n",
    "        # then the model attribute .source to get the path (try printing it out in the console!)\n",
    "        model_uri = version[0].source\n",
    "        # here we load our model\n",
    "        model = mlflow.tensorflow.load_model(model_uri=model_uri)\n",
    "        return model\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90958b41-0f45-46f8-bd4e-7c4e924bd676",
   "metadata": {},
   "source": [
    "Run a `make run_pred` to get the predicition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50de607-7bc7-47a1-9a3f-7a3b303811fa",
   "metadata": {},
   "source": [
    "# Automate the model lifecycle with prefect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed11edc8-5736-486b-aad9-e208ba519e4f",
   "metadata": {},
   "source": [
    "Create e new file called `workflow.py` inside the interface directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae22f37e-dfe7-4074-bf2d-2c6b0309771e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# workflow.py\n",
    "\n",
    "# we take previous functions we wrote\n",
    "from taxifare.interface.main import evaluate, preprocess, train\n",
    "\n",
    "\n",
    "def preprocess_new_data(min_date: str, max_date: str):\n",
    "    # takes the dates as parameters for one month of data\n",
    "    preprocess(min_date=min_date, max_date=max_date)\n",
    "\n",
    "\n",
    "def evaluate_production_model(min_date: str, max_date: str):\n",
    "    # we call the evaluate function to evaluate the production model\n",
    "    # the evaluate functions calls the 'load_model' function so that it loads\n",
    "    #  the in production model so we are evaluating the production model(our default stage is production)\n",
    "    eval_mae = evaluate(min_date=min_date, max_date=max_date)\n",
    "    return eval_mae\n",
    "\n",
    "\n",
    "def re_train(min_date: str, max_date: str):\n",
    "    # retrains model on new data\n",
    "    # updates the split ratio to take 20% of the new month as a validation set\n",
    "    train_mae = train(min_date=min_date, max_date=max_date, split_ratio=0.2) \n",
    "    return train_mae\n",
    "\n",
    "\n",
    "def train_flow():\n",
    "    # realistically this will be a function calculation the time period\n",
    "    min_date = \"2015-01-01\"\n",
    "    max_date = \"2015-02-01\"\n",
    "    preprocess_new_data(min_date, max_date)\n",
    "    old_mae = evaluate_production_model(min_date, max_date)\n",
    "    new_mae = re_train(min_date, max_date)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ad91ee-b282-4572-8b1d-b992b4b0af46",
   "metadata": {},
   "source": [
    "Run the file `python taxifare/interface/workflow.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e4a635-6485-451a-83a2-b5805d2092b8",
   "metadata": {},
   "source": [
    "But right now this is really not doing anything other than running the functions on a new month of data. We need to make this more powerful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd181db2-c1ba-40c1-9bed-5be879e5303d",
   "metadata": {},
   "source": [
    "First we need to conect to prefect. so run `prefect cloud login` in the terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc849ec-e3dd-4fbb-8faf-0617225e89f3",
   "metadata": {},
   "source": [
    "Now that we are conected, we need to change our code to run in the prefect interface as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a05fce-23cc-4696-9a91-c06d68a0f6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from taxifare.interface.main import evaluate, preprocess, train\n",
    "from prefect import task, flow\n",
    "\n",
    "\n",
    "@task\n",
    "def preprocess_new_data(min_date: str, max_date: str):\n",
    "    preprocess(min_date=min_date, max_date=max_date)\n",
    "\n",
    "@task\n",
    "def evaluate_production_model(min_date: str, max_date: str):\n",
    "    eval_mae = evaluate(min_date=min_date, max_date=max_date)\n",
    "    return eval_mae\n",
    "\n",
    "@task\n",
    "def re_train(min_date: str, max_date: str):\n",
    "    train_mae = train(min_date=min_date, max_date=max_date, split_ratio=0.2)\n",
    "    return train_mae\n",
    "\n",
    "@flow\n",
    "def train_flow():\n",
    "    min_date = \"2015-01-01\"\n",
    "    max_date = \"2015-02-01\"\n",
    "    preprocess_new_data(min_date, max_date)\n",
    "    old_mae = evaluate_production_model(min_date, max_date)\n",
    "    new_mae = re_train(min_date, max_date)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19577ea4-d921-44b9-a4a5-a8d621226a0b",
   "metadata": {},
   "source": [
    "Run the file python `taxifare/interface/workflow.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3bc0ec-4ea6-4f7a-bb13-75f4acd01bd6",
   "metadata": {},
   "source": [
    "Now all we need to do is solve the problem we are sugesting which is make 2 tasks run in paralell. For that, all we have to do is go to the `workflow.py` and change the `train_flow` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ea05dc-d3cb-4799-97bc-2cb22cab1029",
   "metadata": {},
   "outputs": [],
   "source": [
    "@flow\n",
    "def train_flow():\n",
    "    min_date = \"2015-01-01\"\n",
    "    max_date = \"2015-02-01\"\n",
    "    processed = preprocess_new_data.submit(min_date, max_date)\n",
    "    old_mae = evaluate_production_model.submit(min_date, max_date)\n",
    "    new_mae = re_train.submit(min_date, max_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a35387-52f3-4f71-ba7e-3756f2b02db5",
   "metadata": {},
   "source": [
    "Run the file python `taxifare/interface/workflow.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706099f7-de2a-4269-9f95-16f8f0037ce2",
   "metadata": {},
   "source": [
    "Now we have all three taks running at the same time! But that is still not what we want. What we need is to parallelize evaluation and training. For that we need to add the following parameter to the functions calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08840e9e-b14d-4cb5-8d1a-43885f8b1362",
   "metadata": {},
   "outputs": [],
   "source": [
    "@flow\n",
    "def train_flow():\n",
    "    min_date = \"2015-01-01\"\n",
    "    max_date = \"2015-02-01\"\n",
    "    processed = preprocess_new_data.submit(min_date, max_date)\n",
    "    old_mae = evaluate_production_model.submit(min_date, max_date, wait_for=[processed])\n",
    "    new_mae = re_train.submit(min_date, max_date, wait_for=[processed])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e532b622-6a2e-4972-a0fc-f34fe6bda446",
   "metadata": {},
   "source": [
    "That's what I'm talking about. Congratulations! You have just Automated your model's lifecycle!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
