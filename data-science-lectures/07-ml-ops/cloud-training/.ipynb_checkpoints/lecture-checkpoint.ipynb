{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17bdb9c0-e338-43b6-80c6-d4054534c097",
   "metadata": {},
   "source": [
    "# Training in the cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb1fbd8-90c2-4057-9879-499eae6c2b02",
   "metadata": {},
   "source": [
    "Good morning, everyone. In this unit training in the cloud. We're looking at moving your mlops workflow from your local machine entirely to the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d05763-239b-4493-9542-6b4967480743",
   "metadata": {},
   "source": [
    "# Plan for the lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e28a277-caa9-407f-a442-adec70bce2bc",
   "metadata": {},
   "source": [
    "So the plan for the lecture is to go through a couple of brief reminders of what you saw in the previous unit. The objective for this unit will look at the cloud platform we've chosen. We'll look at how we can shift application parameters dynamically. We'll look at moving the model to the cloud. We'll look at moving data into the cloud and we'll look at training in the cloud as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818ac897-32c4-4e33-a263-bec436964467",
   "metadata": {},
   "source": [
    "# Reminders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992b99b9-71eb-4bfa-ac54-df1af2d7a264",
   "metadata": {},
   "source": [
    "So a couple of brief reminders. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5468e97e-2599-4422-a96b-5e6b155fab52",
   "metadata": {},
   "source": [
    "# Minimal package structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c57bfc-4c4b-4195-99b9-2bd450155054",
   "metadata": {},
   "source": [
    "We've got a minimal package structure generally with our Python packages, we've got a project root and that's going to have the folder which has the rest of our package in it. Inside that, the first file you'll always find is like a dot python version, or this is a plain text file which says which virtual environment to activate for Python. Then really importantly for us we have the make file which lets us run command lines really quickly, especially longer command line commands. It's really annoying to always either copy or paste them out or type them out. So having the make file to quickly run long command line commands is really useful. We have requirements which describes every single python package needed to run your package. You have Setup.py which describes how to install your package. You then have a folder and inside that folder you have all of the python files you need to install your package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22babc7-32ea-4ca3-a976-cda92b3b2298",
   "metadata": {},
   "source": [
    "# Package installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962b06c9-b9e0-4e23-9360-d891187aea03",
   "metadata": {},
   "source": [
    "So how do you install a package? So you can either do Pip install dot with a final version, but what that will do is take a copy of your folder in its current state and install that and if you make any changes that won't be reflected when you run the package. So generally when you're developing, you always add this dash e editable flag to the command so that if you make a change to the Python file, it's reflected when you run the package as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7efcb5f-f959-4cf8-9f3a-a7e9bdbca07e",
   "metadata": {},
   "source": [
    "# Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec18a2d-3733-4afd-8db4-54522805ef84",
   "metadata": {},
   "source": [
    "So what's our objective for today? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac29bab-73ea-40cf-9c68-a67ebc140918",
   "metadata": {},
   "source": [
    "# Where are we in our journey?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ee38e0-3c3f-445e-9bb3-6015a4be523a",
   "metadata": {},
   "source": [
    "We yesterday in the previous unit took the notebook from the wagon cab team, and then you analyzed it and you converted it into a python package to make it all operable in a script and Python scripts rather than just from a notebook. And then you scaled it to work on the full data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2a1c1b-9da2-465d-9654-94d6126de958",
   "metadata": {},
   "source": [
    "# Train at scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00237735-3fa0-4ab4-951f-de1adb790c8b",
   "metadata": {},
   "source": [
    "All of this, though, was happening on the local machine. So after pulling the initial data, it was stored as a CSV locally. You then ran it through the package and then you stored the model that was trained in the local directory as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c787177a-f5ff-4680-9e4b-ed931aabd25a",
   "metadata": {},
   "source": [
    "# Whats the next step?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7279797-de07-4c8f-999a-357302f89a15",
   "metadata": {},
   "source": [
    "What we want to do next is move it to the cloud. And this is for a few reasons. One, collaboration, if the local machine is the training machine, is really, really difficult. It relies on you pulling your second team member's code down to your machine, adding it to the training, etcetera. Whereas if it's a third independent machine, it's really good. Both of you can interact with it at the same time. You can use a git workflow and you're able to both change and make changes to the production side of things, which is actually doing the training. It stops monopolizing your machine. And what we mean by that is if you have a heavy enough machine learning workload, it will end up taking so much power that it's actually quite hard even to open vs code and properly type. And your time is also part of the overall productivity of a company. So if your training is so heavy that you're not able to code, that's a limitation. And so by moving that weight and that difficulty onto the cloud, you're able to keep coding and keep developing alongside your models being trained. We also want to plug constant real world data. And if that relies on you opening your laptop, that becomes a little bit problematic when you go on holiday, etcetera. Whereas if it's on the cloud, then every single day, every single month, every single year, it can automatically plug this real world data and pull this new data. So putting it on the cloud allows us to do that. And we'll expand on this idea a little bit more in the next unit. Finally, if you don't have a graphical processing unit, a GPU on your machine, your neural networks will train slower. But on the cloud you can rent them really easily. One thing you can do is you could just buy a GPU yourself, but unless you're going to be constantly using it overall, the costs of renting are generally a little bit lower."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f339a93-4d73-4114-bd69-1fe96fa05052",
   "metadata": {},
   "source": [
    "# Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24220a91-79d0-48d5-98c6-80f2a0b00990",
   "metadata": {},
   "source": [
    "So our goal for today is to move the database. So the original data that we train from to the cloud as well. So our process data should be in the cloud and you'll do that in the challenges in this unit. Then we'll go to a virtual machine to do the training instead. And finally, the model should be stored in cloud storage rather than local file storage. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c7bcef-d163-4b73-930c-259fb445ab45",
   "metadata": {},
   "source": [
    "# Transition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac535541-3db3-4437-9c7a-1fb1ca4ebe93",
   "metadata": {},
   "source": [
    "So this is the order that we're going to go in for the lecture. First, we're going to save the trained model to the cloud. Then we're going to change the input so trained from data in the cloud. And then finally, we're going to change the execution and we'll run a training on a virtual machine in the cloud. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a256bfea-6799-4990-a640-22cbce80d9b3",
   "metadata": {},
   "source": [
    "# Transitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d720223a-74fc-4f31-8522-2c8bb49d88e1",
   "metadata": {},
   "source": [
    "So first we'll do the storage of the model. Then we'll do the database and the process data from a table in the cloud. And then finally, we'll look at virtual machines and using a virtual machine in the cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcf60c5-98c7-4a4e-8abc-44e2c9863441",
   "metadata": {},
   "source": [
    "# Choice for source target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ca0f5b-228b-475a-8013-1082b5c086e3",
   "metadata": {},
   "source": [
    "So what are our choices for this storage and this table? So today we're going to use BigQuery and Google Cloud Storage, but there are plenty of other options to provide a similar end product as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e412102d-fc00-4636-b5b4-9f7bfccb0806",
   "metadata": {},
   "source": [
    "# Updated packge structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5c17b7-33d1-4b05-a0a2-23909a5ce6f8",
   "metadata": {},
   "source": [
    "Before we dive into that, let's have a brief look at the updated packet structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d1a80e-eb94-4e68-a283-61fe9096311a",
   "metadata": {},
   "source": [
    "# Updated packge structure 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171e54ea-89bd-4803-8392-6b93edcef02d",
   "metadata": {},
   "source": [
    "So we've got two new files that we'll cover in a little bit. There's an env and an envrc, which provides us our configuration variables and our secrets for our package. We've made a couple of updates to the make file. There's a new command make run train. And this is really, really useful as what this is going to do is directly just run the training command from Main. So instead of having to go to the if name equals main block at the end and comment out the bits you need or don't need, if you just want to run the training, you can type, make, run, underscore, train and we'll see that in a little bit. We've moved from main local to just main. And this is because all the code is now to do with interacting with the cloud. And for today, we and the rest of this module, we've removed the idea of chunking. So you saw yesterday when you would need chunking. Today we're going to presume that most of your workloads, you could just keep scaling up your virtual machine and that would cover most sizable workloads. You could rent 800GB of Ram. Hardly any models are going to need that much more than that. And so for today and the rest of the module, we're going to reduce the complexity and do no more chunking. But if you had to have a particular workload or you had a particular cost limit on your machine, you might want to bring that idea back in for your projects. Then in Logic, there's two important files for today. There's data dot p y, which is loading and storing data from BigQuery and Registry Dot Pi, which is about loading and storing the model. Finally, we'll also have a look at params.py and how that's going to bring dynamic environment variables. And we're going to be able to use that to configure our package depending on its context. So depending on whether it's running on my machine, a fellow developers machine or in the cloud in our production environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f970c37-5964-4c84-b3ee-c7d6e765d71d",
   "metadata": {},
   "source": [
    "# CLoud platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842ef432-ba97-4323-a7d6-6d62d33813a1",
   "metadata": {},
   "source": [
    "Which cloud platform do we pick?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50512ad-9460-4a30-ac05-13839134dcd5",
   "metadata": {},
   "source": [
    "# Google Could Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb3e801-561a-49f4-8ef7-24b1501a6ef8",
   "metadata": {},
   "source": [
    "So you probably could already guess. After you set it up on the first day. But we personally picked Google Cloud platform. What it does is it provides on demand computing resources. So what we mean by that is that if you want a machine and you want it to have four cores and you need 16GB of Ram, you can request it and it can be at your fingertips immediately. In a traditional company, a traditional approach would be if you knew you needed that machine, you'd sort of have to order them six months out and you'd have to predict exactly what you need. But with cloud computing, you have these on demand computing resources. So almost whatever you need, you can rent within a minute, within 30s, etcetera. And you also have this idea of elastic resources. We won't see that today, but we'll see it in the predicting production unit where if you have 1000 users it will scale up a machine big enough for 1000 users. If you have a million users, it will scale up to serve all a million users and you'll pay based on how many users. But there is a bit of cost for them dynamically changing how much you're using. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371f77a2-8a08-46e8-a4c9-655e273d712a",
   "metadata": {},
   "source": [
    "# Service Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98036769-8fb1-4739-b7d9-4f306be57c74",
   "metadata": {},
   "source": [
    " So we got this idea of service layers where you have on premises. So here you would have a huge server rack. You'd predict six months out what you need. You'd have to manage the location. So where you put it in your building, how much hardware you need. You'd have to install the correct operating system and you'd have to install and store the code environment yourself. So what I mean by that is installing Python. Then a slightly more modern approach is what I said just now about being able to demand computing resources from a cloud platform. And on Google, that's Google Compute Engine. And with that you just pay for what you allocate. But after that you don't have to worry about where do you put it? Google will put it wherever they think is most sensible. You don't have to worry about the hardware whether it gets out of date. All you have to do is pick an operating system and still install the environment so it's a little bit easier. Then we have another stage, which is platform as a service, and this will be in the predicting production unit where you use Cloud Run. And here all you have to do is choose the environment for your code. So pick Python, the version you want, etcetera, and run your code. But everything else is sort of managed away from you. And then finally, the most abstracted away is software as a service. So Google BigQuery, where we have no real idea where the machines are running. We don't really know how they run it, but we can interact with the database and use SQL to interact with it. But we're paying for the privilege of using the software. So we'll use Google BigQuery today to be our cloud tables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6b3033-d320-448f-88e1-ff3ae19ab5de",
   "metadata": {},
   "source": [
    "# Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ee6a03-549c-43d2-8cef-e071800dd927",
   "metadata": {},
   "source": [
    "So why Google Cloud platform? It has a product for everything that we personally need. I would say it's the most user friendly of the three main cloud providers of Amazon, Microsoft and Google. So it's a little bit easier to get up and running. It's 20% cheaper than Microsoft Azure, which makes it a bit of a difference. I would say it's also the most mature for machine learning workloads. The most mature platform in general is probably Amazon Web Services. But it's a little bit difficult because it's been around for so long and so they've kept adding more and more new services and it's begun to get a little cumbersome, but for some more niche and more specific things. Amazon Web Services might be where you want to go for. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d790b6cb-17d8-4ff1-850e-c2c871f5a11c",
   "metadata": {},
   "source": [
    "# Interface rule of thumb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa61e98-63af-4cc2-b582-8bdd17edab7b",
   "metadata": {},
   "source": [
    "he rule of thumb. So we need to interact with the cloud platform and there's three main ways to do it. So through the web console that we'll show in a little bit, which is sort of like clicking through a website and asking for what you want like that, it's really great for exploring, so learning what's possible, but it's not ideal if it's a workload that needs to happen often or quickly because you have to click through the website and it can feel a little bit sluggish. Then you have the CLI, the command line interface that you will have set up on setup day. So with Gcloud, Gsutil and BQ, you can interact with all these cloud resources directly from your terminal. It's really great for precise operations. And what we mean by that is when you know exactly what you want to do, it becomes a little bit tricky to find out exactly what all the options are compared to the web console. So the first time you do any cloud operation, I would always recommend using the web console first before jumping into using the CLI from then onwards. Finally, we might want to interact directly through our Python code and we want to do that when we want to automate behaviors. So what we mean by that is, for example, if you need to interact with your cloud storage at 9 a.m. every day in a specific way, you probably want to be able to interact with it through Python and then automatically run that Python script at 9 a.m. every day. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754bb233-71f8-4444-8019-f76f8597f928",
   "metadata": {},
   "source": [
    "# Livecode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3522061-8615-4480-9567-08ded00981c8",
   "metadata": {},
   "source": [
    "So let's quickly go have a look at the Web console, have a quick look at how we get to the services and resources for today. And then we'll have a quick look at the Google application credential environment variable that you set up on setup day and explain how that helps us interact with GCP. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce9938b-03c5-4318-b2fc-586b16ed8472",
   "metadata": {},
   "source": [
    "# Welcome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba90de7e-5f16-4e9d-a809-3abc8fb30dbd",
   "metadata": {},
   "source": [
    "So here is the homepage of Google Cloud platform. What this has is the first stage is an organization if you're part of one, but for most of you, you'll have no organization. But then inside that, we have the idea of a project and a project. My one here, Oliver Bootcamp, is where we store all of our resources related to a specific activity. So in this case it's related to blog and bootcamp and me. And so when I create a table under this, it's within this project. So how do we interact with our services? So one way to navigate to them is the bar at the top here. So we have cloud storage for the model. So cloud storage here, or you can find it on the left here. But if you see, it becomes quite a long list. So it's not loading, but I usually interact with the search bar. I find it a little bit easier, but both are fine where you're going to use big query to store our tables. So this is a SQL workspace and we have our databases and datasets on the side here. And then finally we'll use compute engine in order to produce virtual machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23c0282-8655-4925-bdcc-0535eb186af7",
   "metadata": {},
   "source": [
    "So here is the homepage of Google Cloud platform. What this has is the first stage is an organization if you're part of one, but for most of you, you'll have no organization. But then inside that, we have the idea of a project and a project. My one here, Oliver Bootcamp, is where we store all of our resources related to a specific activity. So in this case it's related to blog and bootcamp and me. And so when I create a table under this, it's within this project. So how do we interact with our services? So one way to navigate to them is the bar at the top here. So we have cloud storage for the model. So cloud storage here, or you can find it on the left here. But if you see, it becomes quite a long list. So it's not loading, but I usually interact with the search bar. I find it a little bit easier, but both are fine where you're going to use big query to store our tables. So this is a SQL workspace and we have our databases and datasets on the side here. And then finally we'll use compute engine in order to produce virtual machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f797f9-6c51-4f6c-bc32-2ff19361496d",
   "metadata": {},
   "source": [
    "# Back to slide - LIVECODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a7c433-c041-42b9-ab13-6daf629c6704",
   "metadata": {},
   "source": [
    "But we also have one more stage of interaction, which is this Google application credentials environment variable. And so what that is, is if you go into your terminal and do echo dollar Google application credentials, it points to a service account key. So you will have downloaded this on the first day. And what this is, is when you try and interact with Google Cloud through Python, it looks for the existence of this environment variable and then it uses the credentials it finds at this path in order to interact. So if you want to interact with BigQuery through Python, you have to make sure that the service account key, which is at this location, has access to your BigQuery and has access to the data sets you want to use. You can also log in using the CLI so you could use this gcloud auth application default login. That would also authenticate your python code. But it's a far more it's a very far reaching credential. So what I mean by that is when you log in this way you don't specifically download a key and say it can have access to this, this and this. It has the full access of your user. So everything you can do through clicking through the website, it has a lot of access and it's generally better in this scenario to download a service account key and pick more specifically what Python has access to. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed35432d-9da5-4b2b-84cd-272f6565d244",
   "metadata": {},
   "source": [
    "# Resource structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e860462e-87f6-4aa3-8fb6-1a259f539f99",
   "metadata": {},
   "source": [
    "So resources are structured underneath projects. So for the live code and for the challenges today within your project, you'll have cloud storage and within cloud storage, it will have buckets. Within BigQuery, we'll have datasets and within compute engine we'll have instances. And then this idea of a project can fall under a folder which can fall under an organization. So in a big organization you might have a folder for your mlops, you might have a folder for your data engineering. And then within those, those might have different projects. So for example, your data lake, your data warehouse, your one machine learning model workload, another machine learning, learning workload, and therefore the resources are associated with that project. So it's generally good to keep the resources associated with a single project together. So for example, in your final project, it's really great to create a new Google Cloud project. So at the end you can delete that project if you want to and it will clear up all the resources associated with that project. So you know how much you're spending on a given project. You know how much you're spending on delivering a given activity through the cloud, and that's why it's important to structure it this way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bf0e7c-2a87-4fef-9dd3-7ff200e8b006",
   "metadata": {},
   "source": [
    "# Identity and access management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960fbfce-e15b-4cfd-89ef-2a6251195547",
   "metadata": {},
   "source": [
    "Then we have this one other idea of this identity and access management Iam where you have a user account. So that's you logged in etcetera and that sort of has full access to everything. But then you have the idea of a service account where you've said specifically what it can access and then you can say the service account should have access to engine, but not to big query. So if someone breaks in, they'll be able to access compute engine, but they're not able to then go further and also access all my data on BigQuery, etcetera. So picking the right level of access can be really, really important. But generally at an organization, it's not really done at a junior level. It's done further and further up because it can have quite far reaching consequences. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8be046-15c6-4193-9103-fb2c7ccb536e",
   "metadata": {},
   "source": [
    "# Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6a7a78-142b-4bb7-a210-22913245960d",
   "metadata": {},
   "source": [
    "So we've talked a little bit about projects. These are a couple of CLI commands you could use to interact with your projects. You also have this idea of regions and zones, so they have different areas that you could deploy your workloads. So for example, do I want my data to be stored in the EU? Do I want it to be stored in the US etcetera? And we'll see that in the lectures today."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35351b1-5a78-4a52-97b2-6e43d19178fc",
   "metadata": {},
   "source": [
    "# Accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6e0838-1b4b-4afd-ab1b-f2284571450e",
   "metadata": {},
   "source": [
    " You have the idea of accounts. So here is a couple of other ways you could log in to your account. So we saw Gcloud auth application default login, but you can also use gcloud init to set up the CLI. But for you all your CLI should already be set up from setup day."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55da50b3-929b-487f-ad14-a3e283814b00",
   "metadata": {},
   "source": [
    "# Budget Alerts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546f3050-e42f-441d-88e0-2ad918feab5e",
   "metadata": {},
   "source": [
    "A budget alert so you can set up a budget alert to work out your resource spending on Google Cloud platform. You'll see today it can end up being very, very expensive. So generally the free tier is quite generous, but if you accidentally demand a resource that's too intensive, for example, a big GPU, etcetera, it's pretty good to set up a budget alert so you don't end up spending too much money. Generally the first time you mess up Google Cloud are pretty happy to refund you. They'd like you to come back as customers, but you wouldn't want to spend 25,000 on renting a huge virtual machine for a month and then not be able to get it back. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fe2f44-8f1e-4e5d-a1c6-24f64dd5cb3f",
   "metadata": {},
   "source": [
    "# Shortcuts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb56701c-b69f-4694-96d6-59ea5f70572e",
   "metadata": {},
   "source": [
    "Here's a couple of shortcuts. So today when we plug into virtual machines, we'll sort of be stuck in their terminal unless we do some more setting up. So to interact with the terminal, we can use Control C to cancel and stop a command control U to erase the current line. Super super nice. Control a to move to the beginning of the current line. Control e to move to the end. And finally option and all. And left and right to move a cursor. Left and right. One word. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034d8ae1-f924-4b73-b17a-cb1b0b0c9720",
   "metadata": {},
   "source": [
    "# Application Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323dd738-f180-445e-90b1-41978e9d827f",
   "metadata": {},
   "source": [
    "So now we have this idea that we want to move everything to the cloud. We also want to shift our Python code to be able to change different parts of it. And that's where this idea of an application parameter is going to come in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a43ca65-a26b-4746-966b-38a4413869d9",
   "metadata": {},
   "source": [
    "# Livecode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5777365-9fb5-4258-b8fd-7b8c391fcaba",
   "metadata": {},
   "source": [
    "So here we want to be able to change the behavior of our package depending on where it's executed. So that means I might want to have my model saved locally while I'm developing locally. But when I push my code to my cloud virtual machine, I want to make sure when it's up there that it's automatically saving the models to the cloud. It's not saving it on its own local file system. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8f90d0-c427-47a8-8435-01cd0cae0c18",
   "metadata": {},
   "source": [
    "# Set up a .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f86ea7-68a0-400d-8437-eebdfd09057e",
   "metadata": {},
   "source": [
    " So to do this, we're first going to create a `.env` file and we'll add a model target variable with the value local. So I'm going to go to my folder and I'm in a version of the taxi fare package for the lecture. I'm going to create a new file called dot env and this has key value pairs where I go model underscore target. And then the right is going to be the value, which is going to be local. So from this, I'm saying save the model locally. \n",
    " \n",
    "`.env`<br>\n",
    "`MODEL_TARGET=local`\n",
    "\n",
    "But we want to be able to load this dynamically and that's where `direnv` comes in. So `direnv` is like `.zshrc`, which you set up on the first day - it runs every time you open a new terminal. Instead. Here, `direnv` runs every time you enter a directory, and it does that with a file called `.envrc`. So if I do `touch .envrc`, you'll see here it says error direnv is blocked. \n",
    "\n",
    "Let's not worry about that for right now. But we'll go in the `.envrc` and in this file we're going to add `dotenv`. So what I'm saying is every time I enter this cloud training directory, I want it to run this command `dotenv` and what `dotenv` does is load the `.env` file. And so then we'll be able to access model target as an environment variable. \n",
    "\n",
    "So here we also need to allow this. So because it allows it to run terminal commands as you enter and enter directories, by default, new `.envrcs` are disabled, so you can't clone a repository online. It keeps you safe in case someone puts a sneaky `.envrc` in and you CD in and they accidentally or deliberately curl some malicious code onto your machine etcetera. So if you go to a new repository, you clone it and you get this message. I'd have a careful look at what's inside the `.envrc` before running `direnv allow`. But as I just put the `.env` command in myself, I feel pretty comfortable doing this. So `direnv allow` and you'll see `export MODEL_TARGET` if I do `echo $MODEL_TARGET`, I'm now able to access the variable as an environment variable. \n",
    "\n",
    "Now, if I go to my `.env` and I update my `MODEL_TARGET=gcs`, for Google Cloud storage. If I do `echo $MODEL_TARGET`, it will still say local for now. So when you make an update to the `.env`, you have to type `direnv reload` to rerun the commands. \n",
    "\n",
    "So now if I go back here, I change back to local and I open this and I type `direnv reload` and `echo $MODEL_TARGET`, I'll get local. So perfect we're able to have our environment variables coming from this `dotenv` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4182d989-bb15-4eba-a530-67c4fdc04257",
   "metadata": {},
   "source": [
    "Then what we're going to do is if we see dot dot, so we exit the folder and then echo dollar model target, you'll see it's not there anymore. So that means if we want to use the same variable names, which might be quite common, such as database password or something like that, we can have different versions of that in different folders, which is really great once you get a lot of projects going. So for now, let's CD back into lecture cloud training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cfd231-7ab9-4b53-87e6-63f2aca4853c",
   "metadata": {},
   "source": [
    "# environent variable demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8304c5f6-a005-4c63-80a8-0dc0553d8030",
   "metadata": {},
   "source": [
    "We now want to see how we could use this in Python. So at the end of registry.py, we'll add a main block to use it. But first we need to load it in params.py. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0009291-adf1-48a5-96b8-6a62d63ebe44",
   "metadata": {},
   "source": [
    "So in order to make a bit of a simplification, we load every single environment variable we need in this file. So here model target equals local. At the moment that's hard coded, but we'll do os.environ dot get and then model underscore target. And from this we'll load the model target environment variable into the model target python variable. \n",
    "\n",
    "`params.py`<br>\n",
    "`MODEL_TARGET = os.environ.get('MODEL_TARGET')`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4401fb30-ee6d-438e-85cb-65a6be91e1f4",
   "metadata": {},
   "source": [
    "Then if we go to logic and register.py, we'll go to the end of the file. We'll just create an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5ee6c1-66c4-425a-923d-2786e7800d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# registry.py\n",
    "# END OF FILE\n",
    "if __name__=='__main__':\n",
    "    # So how did I get this here at the top of every file we're doing from taxi fare,\n",
    "    # dot params import all to import all the parameters. \n",
    "    if MODEL_TARGET == 'gcs':\n",
    "        print('saving to cloud')\n",
    "    else:\n",
    "        print('saving locally')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b606af70-d5bb-4cf6-97c8-d4e42e74d5ed",
   "metadata": {},
   "source": [
    "And what we want here is this code to run when we run it as a script.\n",
    "\n",
    "So when I run this script: `python taxifare/ml_logic/registry.py` it should say saving locally. If we wanna change go to .env, change to gsc, direnv reload and run registry again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de585c9-3f6e-48c1-aa2b-dce7374e12cb",
   "metadata": {},
   "source": [
    "# well done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b325d4c-ca41-4cd5-a8f1-e32c19bfcf2e",
   "metadata": {},
   "source": [
    "So we just saw that we're able to adapt the behavior of the package depending on its execution context. And thanks to this file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de9dd4e-bb60-441b-bff4-40b9c076faed",
   "metadata": {},
   "source": [
    "# theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcebbfa-b047-4c82-9899-2323001cf1fc",
   "metadata": {},
   "source": [
    "We use direnv to load the file and then we loaded it with os.environ into Python and then use that in normal python control flow. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e6ca80-ab24-43a9-a548-1579b1710d8a",
   "metadata": {},
   "source": [
    "# .env Project Config File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d5f72e-c1a3-45dc-91e5-370b30c14ea4",
   "metadata": {},
   "source": [
    "So what is this dotenv file? So you can use it to define behaviors like we just did, but you could also use it generally to store resources such as the path to a database. So here PostgreSQL user. So this gives you all the things you need to access this Postgres database. Other credentials such as API keys, etcetera, that might be personal to you as a user. And therefore because of all these things we generally always want to git ignore it. We don't want it to be put into version control, one for the safety of our secrets, but two, because it should be independent of the code, it should be personal to the environment. So on my production environment in the cloud, it should have its own .env file. And from this the cloud package should be able to define its own behaviors and perform how it wants. But I should be able to change my own local behavior, but without having to change the overall flow of the code. So without having to hard code and change things that are important, I'm able to shift the behavior of my package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dce959-2998-4202-9ce4-fa3ee0ba2d78",
   "metadata": {},
   "source": [
    "# .direnv config loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5738adba-8680-48e7-978b-5d24a8a8d49d",
   "metadata": {},
   "source": [
    "So here we have the few steps are to create a `.env` file. You have variables and they're equal to a value. You've got to make sure to install `direnv`, you need to create a `.env` loader file and in `.envrc` you put the dotenv command to load everything in the `.env` file as environment variables. And then you need to do `direnv allow` to allow that file to work. There's a couple of other commands so here - reload. So when we make an update to the `.env` file we need and reload in order to get our package to behave as we expect. And you can use `direnv status` to check on the status of direnv."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a87a35c-e828-426a-9a62-76c7fb730384",
   "metadata": {},
   "source": [
    "# Environment variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cb54ad-53a6-4733-93c6-d1fa4c73dadd",
   "metadata": {},
   "source": [
    "So once we had the environment variables, we loaded them with the `os.environ.get()`. So `os.environ.get()` is similar to the dictionary. If datasource doesn't exist, it returns none. But we may want it to fail if it doesn't have a value. And so with that we'd use os.environ and square brackets. And then if datasource doesn't exist, the code will break instead. So depending on what behavior you want, you pick one of these two in the command line. We've seen it a few times, but to access an environment variable, you use dollar sign and then the environment variable name. In a Makefile you have to make a small change which is around your environment variables. You have to also add curly braces as well as the dollar sign. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f28d46-9af3-4981-9e75-3329aa16f0c4",
   "metadata": {},
   "source": [
    "# Model in the cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb71b615-b5ee-41a9-a14f-7136467f9322",
   "metadata": {},
   "source": [
    "So now let's actually use that and put the model in the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d571bb33-641d-4c2d-99bb-0055247eb01e",
   "metadata": {},
   "source": [
    "# Legacy Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f9d2be-39cb-4434-bf82-e85b9afa57cb",
   "metadata": {},
   "source": [
    "So still at this point, although we're able to shift the behavior, we've got local data as a CSV, we've got a package and we've got a local directory model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adffc3e2-c0df-49fe-a101-e192498804e9",
   "metadata": {},
   "source": [
    "# Store Target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed2e7cc-eb86-4e24-888a-fe144d0ff585",
   "metadata": {},
   "source": [
    "But we want to be able to store our model on the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a430c9-7448-4e29-a975-403797c76b54",
   "metadata": {},
   "source": [
    "# Google CLoud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00004f9c-60d9-43d2-8f60-51fe4ca116dc",
   "metadata": {},
   "source": [
    "What we're going to use for that is Google Cloud storage, which is static storage from unstructured data. So what I mean by static storage is you can't open the files and edit them. They're saved as they are. And if you want to make a change, what you really have to do is delete it or overwrite it with a new file. You can't open it and add one more line, etcetera. It's static as in it's immutable. Generally, it's where you store your unstructured data. So videos, audio, your raw data from your customers, etcetera. It's the first place you store data and it's generally the cheapest. So models are a great example because generally you save the model at the end of the training, but you're not going to go back and edit that model later. You'll train another model on top of it and you'll save that one as well. So models are a fine example for unstructured data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e82b2a-67ba-4787-86f2-80f585b73006",
   "metadata": {},
   "source": [
    "# Livecode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afae603-d5f3-495e-a411-d959ea20e869",
   "metadata": {},
   "source": [
    " For the live code. We want to push the train models on the cloud on every single train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eeebb66-d05a-412a-924e-31223fd12379",
   "metadata": {},
   "source": [
    "# Save the model to the cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3b8375-878f-4e38-a3fd-466fdce5376a",
   "metadata": {},
   "source": [
    "So first, let's create a bucket, which is the first step of Google Cloud storage, and we'll upload one model ourselves manually. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ed9eae-dc32-4891-b848-efbfb4861028",
   "metadata": {},
   "source": [
    "1. cloud storage\n",
    "2. create\n",
    "3. pick a name (globally unique) -> taxifare-1209\n",
    "4. location EU\n",
    "5. standard\n",
    "6. continue\n",
    "7. create\n",
    "8. confirm\n",
    "9. upload files\n",
    "10. go to models folder\n",
    "11. .lewagon/mlops/training_outputs/models_model file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a43fa65-6a2f-40b6-8273-8642031282d8",
   "metadata": {},
   "source": [
    "Now we need to configure the `.env`:\n",
    "1. by changing the model target to gcs, \n",
    "2. we wanna add our bucket name to params \n",
    "3. we wanna add our bucket name `.env`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc7d740-49be-4a2e-9215-cf8ce7e4065b",
   "metadata": {},
   "source": [
    "- Finally, in `taxifare.registry.py` we wanna checkout the function `save_model`\n",
    "- run a training `make run_train`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f406909b-6acd-442e-b109-e7afb0de1b43",
   "metadata": {},
   "source": [
    "1. copy name of buckt from goggle\n",
    "2. go to .env file and add BUCKET_NAME=test-1209/20230530-033420.h5\n",
    "3. add BUCKET_NAME=os.environ.ge['BUCKET_NAME']\n",
    "4. go to registry explain:\n",
    "\n",
    "Then let's check out registry dot pi. So then go here. \n",
    "\n",
    "So if the model target is GCS we want from Google Cloud to import storage. So this is the model to interact with buckets. We want to get the file name. So this is the date dot h5 to know the date when you train the model and then you create a client. So you just client equals storage dot client. You access the specific bucket with bucket name. And then you have this idea of the blob. So the bucket blob. And that means where we want to put it in the bucket and we want to put it put it at model slash and then the file name. So the date that was trained dot h5. And then finally we call blob dot upload from file name. So we upload it from the file on our local machine to our bucket on the cloud. So let's do that right?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5b2a97-da62-4541-ba33-35ac43d226c1",
   "metadata": {},
   "source": [
    "- so lets run `make run_train`\n",
    "- let's check the console: the model we just trained is save in the models path inside our bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a28577-ad3e-4ae8-af73-598205d244d6",
   "metadata": {},
   "source": [
    "Let me show you how to interact with the bucket using the CLI:<br>\n",
    "- `gsutil ls gs://$BUCKET_NAME/models` -> we can see what is store inside models inside the bucket\n",
    "- `gsutil cp ~/.lewagon/mlops/training_outputs/models/ <> gs://$BUCKET_NAME/models/model-qith-cli.h5` -> we can copy a model into the bucket\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf81486-e912-43a5-96d5-5d2e82dfdfec",
   "metadata": {},
   "source": [
    "#### So now we have achieved out first goal for today - we wer eable to save our model to the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0554d75-d9fd-4442-8daf-bbfbc80cd71d",
   "metadata": {},
   "source": [
    "# Buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89524edc-f21e-4f9c-8999-ab7b40719c04",
   "metadata": {},
   "source": [
    "So buckets are just the overall container for blobs. Here it says no tree structure. And that was what I was mentioning earlier, that the folders are sort of just a fake folder to help us understand. But ultimately it's just a flat structure of files. But some of the files have names, slash names, slash to appear like folders. To interact with it. We can do gsutil and that would just list the buckets or. Like we did to list the blobs inside a folder. Inside a bucket. And then if you wanted to make the buckets yourself on the command line, you can use gsutil and B for make bucket. But generally it's something you don't do that frequently and you probably want to get just right. So don't feel uncomfortable to go back to the UI if you want to see all the options. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2135f1-d5ef-4913-869c-8fd3a6a9edb1",
   "metadata": {},
   "source": [
    "# Blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feba0f96-43f1-4cd1-9b17-63bd94099f1d",
   "metadata": {},
   "source": [
    "A blob is an immutable data storage. So that's what we're saying is that I can't go in and edit a line. I've either got to replace the file with a copy of the file with the extra line added, or I've got to add another version of the file, etcetera, etcetera. So it's best for this unstructured data and models fit into that category. It's called a blob Uri And if you want to dig into a little bit of what the difference is between a Uri or URL is, there's a link here, it's not that important. It's more of a terminology. Here we've got a few more commands that you could look and here we could also use with Star, for example, to copy all the csvs into our bucket, etcetera, etcetera."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885712c4-db23-4235-828b-48d4d3bd7890",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce5d761-5b17-4d83-a6c2-fcf4e7819567",
   "metadata": {},
   "source": [
    "In code we've got two things. So we saw the upload bob code, but there's also the download blob code here. But it's very much the same idea. You have a client, you have a bucket and you have bucket blob. Instead, you're downloading two file name rather than uploading from file name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1466eecf-e575-4afc-b062-6cc37e1b79fa",
   "metadata": {},
   "source": [
    "# Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286c1f17-2ecb-48e9-84bd-e01a8c828e4a",
   "metadata": {},
   "source": [
    "What we saw is we have these buckets that act as containers for our data in the cloud. We have blobs. And each of these store one file, in our case these five files. So "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ed9489-790d-4e54-b5b0-8ba335b63d1d",
   "metadata": {},
   "source": [
    "# Local disk, cloud storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e8d73c-d3bb-4d49-85c3-8cde9deed119",
   "metadata": {},
   "source": [
    "So we have our local disk and that would have folders full of files Instead on the cloud we have a bucket and that's full of blobs and each of those blobs has a link. And you can follow this path exactly to that file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be7ce1b-421a-4b63-8c58-9cb4d6d1346f",
   "metadata": {},
   "source": [
    "# Data in the Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3575ccdd-aace-4201-86f2-7a5d46374493",
   "metadata": {},
   "source": [
    "The next step is going to be to put data in the cloud. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1291335-151a-4432-9c08-26f7375ace12",
   "metadata": {},
   "source": [
    "# Storage target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8a4118-dc26-4c2b-a2b4-2f1be6806390",
   "metadata": {},
   "source": [
    "So we have our model in the cloud, but we want to be able to pull data from somewhere. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badce535-1b6d-4f5f-8909-8b4688691099",
   "metadata": {},
   "source": [
    "# Data Source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a9b550-e911-4d80-9e0a-3e5cefa2193d",
   "metadata": {},
   "source": [
    "This is really important because if I switch to another machine, I don't want to have to re pre-process the data. I want to be able to start working straight away. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7848be54-1da3-4bc4-b7dd-0b97c69c5f88",
   "metadata": {},
   "source": [
    "# Google Big Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1107815b-3022-4c84-ae89-d987ef00865f",
   "metadata": {},
   "source": [
    "And what we're going to use for that is Google BigQuery. So we have our data warehouse, Google BigQuery, which is where we store all of our data. It's really great for scaling up to enormous data loads. So we're not going to do that this week for time reasons. But they always say that Google BigQuery can handle petabytes worth of data. So the next stage up from a terabyte and it just keeps scaling up and up. There's almost no company which data can't be handled by Google BigQuery and it really helps a lot. It has a couple of built in ML capabilities as well, so you can train deep learning models in SQL. I probably wouldn't recommend it. It feels a little bit rough, but for quickly doing linear regressions, it's definitely something worth digging into, especially on large amounts of data. It can be pretty powerful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc5441f-6540-4212-ab53-0b7070bd6297",
   "metadata": {},
   "source": [
    "# Load table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80df7748-1fa5-455c-aec3-2b8f77fd6eb1",
   "metadata": {},
   "source": [
    "So we'll have a quick look at how to load a table from BigQuery into a Pandas dataframe. So we have from Google Cloud import BigQuery, you got a project, you've got a dataset. So a dataset is just a storage idea. And inside the dataset you have many tables which are SQL tables. So in SQLite, your equivalent would be you had an SQLite database and that had many tables inside of it. In BigQuery you have datasets that are related to tables and then inside that you have many tables. Then we can write our query in an F string. We instantiate the client so similar with Google Cloud storage, you make a query job where you query with the client and you get a result. So you were doing this for the raw data yesterday. But what we need now is to upload our database and that becomes a little bit more difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3158512-a7d6-4c30-bc26-313d0f5791a7",
   "metadata": {},
   "source": [
    "# Upload dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b191d290-71d3-4e82-a576-24191cfe73b5",
   "metadata": {},
   "source": [
    "So I'll take this code here. We have a project, our data set a table, and we've got a dummy data frame with just two columns with two rows in. You instantiate the client and then you have to put a write mode. So I'll show you exactly what this means. You put a load job config and you put the write mode into that and then you do client dot load table from data frame. You put the DF, you put the table that you want to load it to. So above here, which is project and then dot dataset, dot table and you put a job config and then we'll load it directly to BigQuery. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad93c85a-271f-4448-a06a-cebc959508d7",
   "metadata": {},
   "source": [
    "# Livecode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317929fd-9845-4751-8568-d3f131c79529",
   "metadata": {},
   "source": [
    "So let's see that right now. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2540650-1ced-41b5-83f8-336d0982047d",
   "metadata": {},
   "source": [
    "# training from the cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24237cd6-9c46-4aa0-b652-37251316b1d5",
   "metadata": {},
   "source": [
    "So we have to create the data set and then we'll create a Python script and we'll use that code. So we'll go here. We'll go to Big Query.\n",
    "- create a datase inside project called batch-1209\n",
    "- copy code form slide\n",
    "- create a file called upload.py inside taxifare\n",
    "- change project to: taxifare-379314\n",
    "- change dataset to: cloud_training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3977ddf-242d-41ed-9c69-fc46f6033bf8",
   "metadata": {},
   "source": [
    "But now let's have a quick look at what that right mode means. So the right mode is right truncate at the moment. So what that means is if I run it again, it's going to truncate i.e. get rid of the table and then put it back in again. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7b63e8-8608-4c3d-b9ef-46936a45a42d",
   "metadata": {},
   "source": [
    "But if I change the write mode to write append. What I'll say is that instead of deleting it, it will add again, two columns, one and two. So depending on your workload, you might want an append where it adds new data to the end of the table. Or you might want truncate where it just creates a brand new table with a fresh set of data. If you've changed your processing, for example. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802b8e26-fb64-46db-8d94-1f7d33b59a97",
   "metadata": {},
   "source": [
    "So here we go. So now we know how we can upload stuff to BigQuery and then the challenges. Today you'll upload the process dataset directly to BigQuery for us for the challenge already in this folder today, we've actually been pulling directly from BigQuery. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533ac435-2d46-49a6-ace9-95e6f71323a9",
   "metadata": {},
   "source": [
    "- GO TO MAIN.PY LINE 89<br>\n",
    "So here we've been getting our data. When we do trade, we've actually been pulling it from our process dataset already. So during the lecture we've actually already been pulling from a Pre-processed dataset on BigQuery, and you'll pull your you'll create your own dataset and pull from that dataset during the challenges today. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee95cca-8d25-4d4d-940f-3c2935d32872",
   "metadata": {},
   "source": [
    "# Teory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6934cbbb-ab20-478d-a4e4-7f0d5d43e909",
   "metadata": {},
   "source": [
    "So  what we've looked at is we've got these data warehouses in the cloud. They're full of datasets which inside them have tables which are just SQL tables that store our data for us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900e9252-5dfb-46b6-8892-095c4af7a6cd",
   "metadata": {},
   "source": [
    "# Datasets and tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc10723-85cb-46aa-87b8-6c72d17f8dbc",
   "metadata": {},
   "source": [
    "How we can interact with them. So generally with BigQuery, you interact with code SQL code or Python code. You can also interact with the command line interface. So we could do the list, all the data set data dataset to list all the tables you could do BQ show, but it looks a little bit ugly if it's full of data. So I'd say generally for big query, the CLI is not that amazing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b966108f-d81b-4713-b726-5b9d395a3ddb",
   "metadata": {},
   "source": [
    "# imports and queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5a38d0-8e56-439e-a78e-b9312a5a5e5b",
   "metadata": {},
   "source": [
    "except for this one command, which I'd say is the one used commonly, which is BQ load. So here I could load directly to a table from a CSV without having to go through Python. So this BQ load command can be really, really powerful. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7bd653-8893-4452-b4d6-8e43eecfa592",
   "metadata": {},
   "source": [
    "# Query Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8852d072-d0de-439e-98fe-7b3997bcb321",
   "metadata": {},
   "source": [
    "One thing to note is that unlike a traditional database with big query, you pay per scan row. So instead of paying for the machines that you're using, you pay for the complexity of your queries. So here to check that out, you can use dry run and evaluate the cost of a query. But the idea is if it has a lot of rows and a lot of orders buys and a lot of sort buys, your cost will go up a lot more than a simple select statement. So how complicated your SQL queries are affect how much you pay, how regularly you do the queries affects how much you pay. So that's why it's not really the best database if you wanted to query it every single second. So for connecting a database to a website, you'd probably pick something different than BigQuery. But for doing our daily trading or monthly training, etcetera, the query cost is perfectly manageable and once again it has quite a generous free tier, even beyond your free credits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1f9008-8007-41b4-8aa2-ccc88338a169",
   "metadata": {},
   "source": [
    "# Training in the cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d3c26f-4c2e-4271-8626-70d28f3cb5b3",
   "metadata": {},
   "source": [
    "Here. We also want to do training in the cloud. So how are we going to achieve that is using virtual machines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bd86fd-46b2-4518-b849-89a1cd704cb6",
   "metadata": {},
   "source": [
    "# Data Source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a321f453-cf82-45ba-b14b-ee5355868e1a",
   "metadata": {},
   "source": [
    "So we've got our database which is pulling from tables, we've got our storage for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73080ff-0e92-458e-8f4e-b7acb80d17c3",
   "metadata": {},
   "source": [
    "# Cloud Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ca3390-2e0f-4e06-9df8-f9e6e73968ea",
   "metadata": {},
   "source": [
    "And now finally we're going to put the piece in the middle and move from my local computer to a virtual machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3118abd6-d02a-4caf-972e-6db2b8c04b0c",
   "metadata": {},
   "source": [
    "# Livecode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c825ccf-bb75-40e4-b06d-6efe37f0ef4b",
   "metadata": {},
   "source": [
    " So we want to run the package training in the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04c3438-b13e-4b23-b531-9310ff7ad017",
   "metadata": {},
   "source": [
    "# Create a virtual machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ae544c-fbf7-49b1-af50-f5d6f42a1cf5",
   "metadata": {},
   "source": [
    "So let's create a virtual machine. And once again, we'll start in the web console. We can look at all the different options and we'll create an Ubuntu virtual machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f086a6-ba69-49c4-ab40-2e5e150f2a70",
   "metadata": {},
   "source": [
    "- compute engine\n",
    "- create instance\n",
    "- name it\n",
    "- europe-west1\n",
    "- E2 STANDARD2\n",
    "- BOOT DISK UBUBTU X86/64 2204LTS\n",
    "- 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc910e3-a16b-497c-ab48-49183bd3ab9a",
   "metadata": {},
   "source": [
    "So during the challenge today, you'll set one up yourself to access the taxi fare package and to install everything you need for the taxi fare package. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
