{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a5bcfd7-d7d2-41ee-8404-25ecff1c356c",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43db25d8-874b-47d8-85b6-6dca10f5bc49",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SCORERS' from 'sklearn.metrics' (/home/tatchiwiggers/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/sklearn/metrics/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline, FeatureUnion, make_pipeline, make_union\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler, OneHotEncoder, FunctionTransformer\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtpot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TPOTRegressor\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtempfile\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mkdtemp\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mshutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rmtree\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/tpot/__init__.py:27\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"This file is part of the TPOT library.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03mTPOT was primarily developed at the University of Pennsylvania by:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtpot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TPOTClassifier, TPOTRegressor\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdriver\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m main\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/tpot/tpot.py:31\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _safe_indexing\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TPOTBase\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassifier\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classifier_config_dict\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregressor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m regressor_config_dict\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/tpot/base.py:82\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassifier_cuml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classifier_config_cuml\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregressor_cuml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m regressor_config_cuml\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SCORERS\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgp_types\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Output_Array\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgp_deap\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     85\u001b[0m     eaMuPlusLambda,\n\u001b[1;32m     86\u001b[0m     mutNodeReplacement,\n\u001b[1;32m     87\u001b[0m     _wrapped_cross_val_score,\n\u001b[1;32m     88\u001b[0m     cxOnePoint,\n\u001b[1;32m     89\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/tpot/metrics.py:27\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"This file is part of the TPOT library.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03mTPOT was primarily developed at the University of Pennsylvania by:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_scorer, SCORERS\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbalanced_accuracy\u001b[39m(y_true, y_pred):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124;03m\"\"\"Default scoring function: balanced accuracy.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    Balanced accuracy computes each class' accuracy on a per-class basis using a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m        0.5 is as good as chance, and 1.0 is perfect predictive accuracy\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'SCORERS' from 'sklearn.metrics' (/home/tatchiwiggers/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/sklearn/metrics/__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer, make_column_selector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline, make_union\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "\n",
    "from tpot import TPOTRegressor\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bf6a01-b2f9-47db-85ce-ec3bca1ad110",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting the display for Pipelines and ColumnTransformers\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e9c033-e040-4bb8-8fd8-53f5e6288310",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04170299-18ab-4013-b966-5305237c5880",
   "metadata": {},
   "source": [
    "ðŸŽ¯ We are going to predict the charges of a health insurance contract based on various features using the following dataset.\n",
    "\n",
    "ðŸ’¾ Download the dataset [here](https://wagon-public-datasets.s3.amazonaws.com/data_workflow.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2169db2-1336-4e95-a7c6-6637d8d765fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/data_workflow.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfbad69-fbd4-4d68-ad62-166c081d9efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032f5fb6-1e4d-4215-aefd-e2b438423487",
   "metadata": {},
   "source": [
    "Now let's define our `X` and out `y`, so that we have something to work with. We will also do a quick manual Holdout Method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90397cac-fa00-41eb-980b-d09b2e8db1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns='charges')\n",
    "y = data['charges']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857c1512-9549-4331-87ac-0570ea05beb1",
   "metadata": {
    "tags": []
   },
   "source": [
    "Today we will: <br>\n",
    "1. Impute missing values\n",
    "2. Scale numerical features\n",
    "3. Encode categorical features\n",
    "4. Fine-tune our model and our preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef35190-2336-4ca7-8b14-4f62282fdd43",
   "metadata": {},
   "source": [
    "All at once, in one single cell!! :D "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa2ec6b-a9c7-4385-9093-ec05b6adc918",
   "metadata": {},
   "source": [
    "## Preprocessing Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb976dc-e32f-42ed-831d-54562064d7df",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f90c9d5-9e41-47e4-b27f-4287f7de8bfa",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb76c49f-a0d1-435a-9f33-5dae970eeb55",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Preprocess the \"age\" column\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('standard_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train[['age']])\n",
    "pipeline.transform(X_train[['age']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d55fa9-da18-4a4e-87ae-566096298e62",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can now look at our pipeline, as well as access individual steps of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80de36c-84e3-4fb5-a323-eac69425db4c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4cae85-8659-4b8c-b146-2131153929ad",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(pipeline[1]) # by index\n",
    "print(pipeline['standard_scaler']) # by name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fe012b",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Column Transformer â‘‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1ec819",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Column Transformers allow you to apply specific changes to specific columns in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c36d66",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For this one we will:<br>\n",
    "    1. Impute and scale numerical values<br>\n",
    "    2. Encode categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f6d5b6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Impute then scale numerical values: \n",
    "num_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"mean\")),\n",
    "    ('standard_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Encode categorical values\n",
    "cat_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Parallelize \"num_transformer\" and \"cat_transfomer\"\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num_transformer', num_transformer, ['age', 'bmi']),\n",
    "    ('cat_transformer', cat_transformer, ['smoker', 'region'])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33aef74",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's checkout out our CT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c03716",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f02abb",
   "metadata": {
    "hidden": true
   },
   "source": [
    "That's cool but how do we use it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0b0209",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "#Using display instead of print to see a proper DF \n",
    "# in the form of a table with borders around rows and columns\n",
    "print(\"Original training set\")\n",
    "display(X_train.head(3))\n",
    "\n",
    "print(\"Preprocessed training set - Notice how we are missing feature names!\")\n",
    "display(pd.DataFrame(data=X_train_transformed).head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a665f0f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "preprocessor.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17e4d19",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    data = X_train_transformed,\n",
    "    columns=preprocessor.get_feature_names_out()\n",
    ").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149137de",
   "metadata": {
    "hidden": true
   },
   "source": [
    "what about the children? We can't leave them behind! :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e53155",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Making sure the children are safe and sound\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num_transformer', num_transformer, ['age','bmi']),\n",
    "    ('cat_transformer', cat_transformer, ['region','smoker'])],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173c3ed3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    preprocessor.fit_transform(X_train),\n",
    "    columns=preprocessor.get_feature_names_out()\n",
    ").head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb34279",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Custom: Function Transformer â†’"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4080abee",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If you'd like to do you r own thing, you can use custom transformers. These encapsulate a Python function into a transformer object. As it is a transformer, it still works with Pipelines and other transformers, like the ColumnTransformer we just saw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329f62a4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Create a transformer that compresses data to 2 digits (for instance!)\n",
    "# rounder = FunctionTransformer(np.round)\n",
    "\n",
    "# We can use a lambda function for more customizable functions\n",
    "rounder = FunctionTransformer(lambda array: np.round(array, decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251d6bbd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Add it at the end of our numerical transformer\n",
    "num_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('rounder', rounder)])\n",
    "\n",
    "# Encode categorical values\n",
    "cat_transformer = OneHotEncoder(drop='if_binary',\n",
    "                                handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num_transformer', num_transformer, ['bmi', 'age']),\n",
    "    ('cat_transformer', cat_transformer, ['region', 'smoker'])],\n",
    "    remainder='passthrough')\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b32273e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(preprocessor.fit_transform(X_train)).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe14127f",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Feature Union ||"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368c50d6",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can take this a step further! By applying entire transformers in parallel.<br>\n",
    "yes we can! Let's do that and create a nwe feature as well: `bmi_age_ratio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d450bde5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46ad217",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Create a custom transformer that multiplies/divides two columns\n",
    "# Notice that we are creating this new feature completely randomly just as an example\n",
    "bmi_age_ratio_constructor = FunctionTransformer(lambda df: pd.DataFrame(df[\"bmi\"] / df[\"age\"]))\n",
    "\n",
    "union = FeatureUnion([\n",
    "    ('preprocess', preprocessor), # columns 0-7\n",
    "    ('bmi_age_ratio', bmi_age_ratio_constructor) # new column 8\n",
    "])\n",
    "\n",
    "union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e7a77b",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Summary with make_*** shortcuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302b67e0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.pipeline import make_union\n",
    "# from sklearn.compose import make_column_transformer\n",
    "\n",
    "Pipeline([\n",
    "    ('my_name_for_the_imputer', SimpleImputer()),\n",
    "    ('my_name_for_the_scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa3b2e1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "make_pipeline(SimpleImputer(), StandardScaler()) # here we dont get to name our steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b8641b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's try and redo it all with this new method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41305b9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_transformer = make_pipeline(SimpleImputer(), StandardScaler())\n",
    "cat_transformer = OneHotEncoder()\n",
    "\n",
    "preproc_basic = make_column_transformer(\n",
    "    (num_transformer, ['age', 'bmi']),\n",
    "    (cat_transformer, ['smoker', 'region']),\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "preproc_full = make_union(preproc_basic, bmi_age_ratio_constructor)\n",
    "preproc_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192b1f6a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Cool right? The upside is that it's much faster to write and implement, but the big downside is that you can't select you own names for transformers, which may be something that you care about doing.<br>\n",
    "We can also use `make_column_selector`to select features to be used based on their `dtype`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aa5100",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.compose import make_column_selector\n",
    "\n",
    "num_col = make_column_selector(dtype_include=['float64'])\n",
    "cat_col = make_column_selector(dtype_include=['object','bool'])\n",
    "\n",
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151daf3c",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Complete Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80414063",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.compose import make_column_selector\n",
    "\n",
    "#IMpute then scale numerical values\n",
    "num_transformer = make_pipeline(SimpleImputer(), StandardScaler())\n",
    "num_col = make_column_selector(dtype_include=['float64'])\n",
    "\n",
    "# Encode categorical values\n",
    "cat_transformer = OneHotEncoder()\n",
    "cat_col = make_column_selector(dtype_include=['object','bool'])\n",
    "\n",
    "# Use the ColumnTransformer to parallelize these operations\n",
    "preproc_basic = make_column_transformer(\n",
    "    (num_transformer, num_col),\n",
    "    (cat_transformer, cat_col),\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "preproc_full = make_union(preproc_basic, bmi_age_ratio_constructor)\n",
    "preproc_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27de549",
   "metadata": {},
   "source": [
    "## Including Models in Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a81123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import Ridge\n",
    "\n",
    "# Preprocessor -Impute then scala numerical values\n",
    "num_transformer = make_pipeline(SimpleImputer(), StandardScaler())\n",
    "# encode cateforical values\n",
    "cat_transformer = OneHotEncoder()\n",
    "\n",
    "# use the ColumnTransformer to parallelize these operations\n",
    "preproc = make_column_transformer(\n",
    "    (num_transformer, make_column_selector(dtype_include=['float64'])),\n",
    "    (cat_transformer, make_column_selector(dtype_include=['object','bool'])),\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Add estimator\n",
    "pipeline = make_pipeline(preproc, Ridge())\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff7631-8743-4be0-b1c4-6dabdd925b8a",
   "metadata": {},
   "source": [
    "Because my pipeline inherits from my last step, i can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ce61d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Pipeline\n",
    "pipeline.fit(X_train,y_train)\n",
    "\n",
    "# Make predictions\n",
    "pipeline.predict(X_test.iloc[0:1])\n",
    "\n",
    "# Score model\n",
    "pipeline.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d61aad3",
   "metadata": {},
   "source": [
    "### Cross-validate Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493cb8d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(pipeline, X_train, y_train, cv=5, scoring='r2').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab90809d",
   "metadata": {},
   "source": [
    "### Gridsearch a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d7c8df",
   "metadata": {},
   "source": [
    "Believe it.. Grid Search is also possible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a680c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which parameters of the pipeline are GridSearch-able? here you can look for the params you wanna grid search\n",
    "# pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60606717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# instantiate a GS\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline, # instead of our model\n",
    "    param_grid={\n",
    "        # we can access ANY component of the Pipeline\n",
    "        # and ANY available hyperparamater you want to optimize - These are double\n",
    "        # underscores also referred to as \"dunder\"\n",
    "        #step              #pipeline  #transformer   #strat\n",
    "        'columntransformer__pipeline__simpleimputer__strategy': ['mean', 'median'],\n",
    "        'ridge__alpha': [0.1, 0.5, 1, 5, 10]\n",
    "    },\n",
    "    cv=5,\n",
    "    scoring=\"r2\")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718b7a6d",
   "metadata": {},
   "source": [
    "ðŸ’¾ Let's save the pipelined model with the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2ac1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_tuned = grid_search.best_estimator_\n",
    "pipeline_tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781e8a86",
   "metadata": {},
   "source": [
    "ðŸ”® We can use this \"best\" model for predictions without re-training it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb0d5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_tuned.predict(X_test[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9733d324",
   "metadata": {},
   "source": [
    "An extra tip for you guys:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea89041",
   "metadata": {},
   "source": [
    "### Cache to avoid repeated computations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0d40a2",
   "metadata": {},
   "source": [
    "Are your preprocessing steps too long to run?\n",
    "\n",
    "You can use caching techniques!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79737cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tempfile import mkdtemp\n",
    "# from shutil import rmtree\n",
    "\n",
    "# Create a temp folder\n",
    "cache_dir = mkdtemp()\n",
    "\n",
    "# Instantiate the Pipeline with the cache parameter\n",
    "pipeline = make_pipeline(preproc_basic, Ridge(), memory=cache_dir)\n",
    "\n",
    "#instantiate a GS\n",
    "# instantiate a GS\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline, \n",
    "    param_grid={\n",
    "        # Access any component of the Pipeline\n",
    "        # and ANY available hyperparamater you want to optimize - These are double\n",
    "        # underscores also referred to as \"dunder\"\n",
    "        'columntransformer__pipeline__simpleimputer__strategy': ['mean', 'median'],\n",
    "        'ridge__alpha': [0.1, 0.5, 1, 5, 10]\n",
    "    },\n",
    "    cv=5,\n",
    "    scoring=\"r2\")\n",
    "# Clear the cache directory after the cross-validation\n",
    "rmtree(cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca93f775",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3a860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the directory after the CV\n",
    "rmtree(cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac26e38",
   "metadata": {},
   "source": [
    "###  Debug your pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f72ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the components of a Pipeline with `named_steps`\n",
    "pipeline_tuned.named_steps.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656cfe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check intermediate steps\n",
    "print(\"Before preprocessing, X_train.shape = \")\n",
    "print(X_train.shape)\n",
    "print(\"After preprocessing, X_train_preprocessed.shape = \")\n",
    "pipeline_tuned.named_steps[\"columntransformer\"].fit_transform(X_train).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c527ef25",
   "metadata": {},
   "source": [
    "### Exporting models/Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca2d5c9",
   "metadata": {},
   "source": [
    "ðŸ’¾ You can export your final model/pipeline as a pickle file so you can load them into your python file or your notebook so you can use it directly as it is.\n",
    "\n",
    "ðŸ‘‰The file can then be loaded back into a notebook or deployed on a server (see ML Ops module)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a429980e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# Export Pipeline as pickle file\n",
    "with open(\"pipeline.pkl\", \"wb\") as file:\n",
    "    pickle.dump(pipeline_tuned, file)\n",
    "\n",
    "# Load Pipeline from pickle file\n",
    "my_pipeline = pickle.load(open(\"pipeline.pkl\",\"rb\"))\n",
    "\n",
    "my_pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2790846",
   "metadata": {},
   "source": [
    "# That cool thing I told you about ðŸŽ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e611944",
   "metadata": {},
   "source": [
    "**Auto ML**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d4ac8b-6bb3-4dd9-b72e-5d502031f967",
   "metadata": {},
   "source": [
    "So here we have our training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adcfcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from tpot import TPOTRegressor\n",
    "\n",
    "X_train_preproc = preproc_basic.fit_transform(X_train)\n",
    "X_test_preproc = preproc_basic.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dc8f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate TPOTClassifier\n",
    "                    #like epochs    #how much info\n",
    "tpot = TPOTRegressor(generations=4, population_size=20, verbosity=2, scoring='r2', n_jobs=-1, cv=2)\n",
    "\n",
    "# Process autoML with TPOT\n",
    "tpot.fit(X_train_preproc, y_train)\n",
    "\n",
    "# Print score\n",
    "print(tpot.score(X_test_preproc, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4058a33-5a4b-4129-bad4-a25aa8f4c047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export TPOT Pipeline to a Python file\n",
    "tpot.export(os.path.join(os.getcwd(),'tpot_iris_pipeline.py'))\n",
    "\n",
    "! cat 'tpot_iris_pipeline.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ab89d9-cc50-4e68-b064-0a5ff40dc3d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
