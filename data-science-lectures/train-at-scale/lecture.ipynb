{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2786f99-5cdb-4700-899f-9ed43e858995",
   "metadata": {},
   "source": [
    "# ML Ops Module\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07025287-0f9d-46b6-88b5-228fb95c475a",
   "metadata": {},
   "source": [
    "# ðŸš— Welcome to WagonCab ðŸš—"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54d57f1-a12b-407b-9318-853fec61ff30",
   "metadata": {},
   "source": [
    "# But what is a ML Engineer, exactly?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cee4bf-dee0-449a-b6a8-7c3fca4283d4",
   "metadata": {},
   "source": [
    "A Machine Learning (ML) Engineer is a professional who is involved in the development, deployment, and maintenance of machine learning systems. They combine knowledge of software engineering and machine learning to create ML models and systems that can be used to provide data-driven solutions. \n",
    "\n",
    "Some key responsibilities of an ML engineer are:\n",
    "\n",
    "1. **Data Analysis**: ML engineers work with large amounts of data. They are responsible for cleaning, processing, and analyzing this data to extract meaningful insights that can be used to improve machine learning models.\n",
    "\n",
    "2. **Model Development**: ML engineers use various machine learning algorithms and models to build ML systems. They select the appropriate model, train it using the available data, and then evaluate its performance.\n",
    "\n",
    "3. **Software Engineering**: Unlike data scientists, who are primarily focused on the development of models, ML engineers also need to have strong software engineering skills. They need to be proficient in various programming languages such as Python, Java, or R, and they often need to write production-level code.\n",
    "\n",
    "4. **System Design**: ML engineers design and build machine learning pipelines, which involve data ingestion, data transformation, model training, and model deployment. They must ensure that these systems are scalable and robust.\n",
    "\n",
    "5. **Testing and Validation**: ML engineers are also responsible for validating the results of machine learning models. They ensure that the models are performing as expected and are reliable and secure.\n",
    "\n",
    "6. **Deployment and Maintenance**: After the development and testing phase, ML engineers deploy the machine learning models into the production environment. They then monitor the performance of these models over time, fine-tune them as necessary, and maintain the overall system.\n",
    "\n",
    "7. **Communication**: ML engineers often need to collaborate with other teams in the organization, including data scientists, software engineers, product managers, and sometimes even stakeholders. They need to be able to explain complex concepts in a way that non-technical team members can understand.\n",
    "\n",
    "In terms of skills, an ML engineer would typically have a strong background in computer science and mathematics, along with expertise in machine learning algorithms and principles, proficiency in programming languages like Python, experience with big data platforms and tools, and familiarity with machine learning frameworks like TensorFlow or PyTorch. \n",
    "\n",
    "It's also increasingly common for ML engineers to have knowledge of Machine Learning Operations (MLOps), a practice that brings together ML, DevOps, and data engineering to automate and improve the process of machine learning model development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff40954-7776-4a0f-b0e3-26380d9a4acc",
   "metadata": {},
   "source": [
    "A data scientist, on the other hand, is a professional who uses their knowledge in statistics, data analysis, and machine learning to extract insights and knowledge from complex and large volumes of data. The role of a data scientist can vary significantly across different organizations, but here are some common responsibilities:\n",
    "\n",
    "1. **Data Analysis**: A major part of a data scientist's job is to analyze data to extract insights. This can involve data preprocessing, cleaning, and feature engineering to prepare data for analysis. They also interpret the data and provide insights that can be used for decision-making.\n",
    "\n",
    "2. **Model Development and Machine Learning**: Data scientists create predictive models using machine learning algorithms. They select appropriate models, train them using available data, evaluate their performance, and fine-tune them to improve their accuracy.\n",
    "\n",
    "3. **Data Visualization**: They present data in a graphical format that makes the information easy to understand. This could be for internal use, to help stakeholders understand the data and the insights derived from it.\n",
    "\n",
    "4. **Communication**: Data scientists need to communicate complex data insights to non-technical team members and stakeholders. This can involve explaining the data, the methodology used, the conclusions drawn, and the implications for the organization.\n",
    "\n",
    "5. **Decision-Making**: Based on the insights drawn from data, data scientists advise on decision-making processes in various business areas, such as product development, marketing strategy, or operational efficiency.\n",
    "\n",
    "In terms of skills, data scientists typically have a strong background in mathematics and statistics, expertise in a programming language (commonly Python or R), experience with databases and SQL, knowledge of machine learning algorithms and principles, and proficiency in data visualization tools and techniques.\n",
    "\n",
    "While there is some overlap between data scientists and ML engineers, data scientists usually focus more on analysis and insight extraction, and they often work with smaller datasets and less emphasis on production code. Where, ML engineers typically work on deploying machine learning models at scale, often dealing with larger datasets and production systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac552ca0-04b5-4e76-87b4-7394ec7f8fa1",
   "metadata": {},
   "source": [
    "# ML Infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dfca9a-3833-4099-b3e2-9a775e47d3d6",
   "metadata": {},
   "source": [
    "Machine learning infrastructure refers to the tools, systems, and technologies that are used to develop, deploy, and maintain machine learning models in a scalable and robust manner. It is the backbone that supports the end-to-end lifecycle of a machine learning project, from data collection and preprocessing, to training and serving models, to monitoring model performance. \n",
    "\n",
    "I'll just explain each of those steps in the context of a typical machine learning project:\n",
    "\n",
    "1. **Data Transformation**: Before a machine learning model can be trained, the data used for training must be properly formatted and cleaned. This process is known as data transformation, and it often includes steps like:\n",
    "\n",
    "   - **Data Cleaning**: Removing or correcting erroneous data, dealing with missing values, and eliminating duplicates.\n",
    "   \n",
    "   - **Feature Engineering**: Creating new features from existing ones to better represent the underlying patterns in the data. This might involve tasks like one-hot encoding for categorical variables, normalization or standardization of numerical variables and so on...\n",
    "   \n",
    "   - **Data Splitting**: Dividing the dataset into training, validation, and test sets. The training set is used to train the model, the validation set is used to tune hyperparameters and make decisions on the model design, and the test set is used to evaluate the model's performance on unseen data.\n",
    "\n",
    "2. **Model Training and Development**: Once the data is ready, a machine learning model can be trained. This involves choosing an appropriate algorithm (like linear regression, decision trees, neural networks, etc.), and feeding the training data into this algorithm. The algorithm then 'learns' from this data by adjusting its parameters to minimize the discrepancy between its predictions and the actual values. This discrepancy is measured by a loss function, which the model aims to minimize.\n",
    "\n",
    "3. **Model Inference**: After a model has been trained, it can be used to make predictions on new, unseen data. This process is called model inference. For instance, if you've trained a model to predict house prices based on features like size, location, and number of bedrooms, you could use this trained model to predict the price of a house that wasn't in your original training data.\n",
    "\n",
    "4. **Integration**: Once a model has been trained and validated, it needs to be integrated into the larger system or application where it will be used. This could involve deploying the model to a server, setting up an API for other services to use the model, or integrating the model into a software application. This step also involves ensuring that the model can handle the scale of requests it will receive in a production environment, and setting up processes to monitor the model's performance and update or retrain the model as needed.\n",
    "\n",
    "Each of these steps is crucial to creating a useful machine learning model. Mistakes or oversights at any step can significantly affect the model's performance, so each one needs to be carried out carefully and thoroughly.\n",
    "\n",
    "\n",
    "\n",
    "Creating a robust machine learning infrastructure involves a combination of many or all of these components, and it often involves making trade-offs based on the specific needs and constraints of the organization or project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1cf769-0272-453e-bc51-0c63ceadf866",
   "metadata": {},
   "source": [
    "# ðŸš— Back to WagonCab ðŸš—"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95461f7-7b3b-4d9a-ace2-85545946f9d8",
   "metadata": {},
   "source": [
    "# Pyen vs Venv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fe5a11-50dc-4a99-88b2-048d4500909c",
   "metadata": {},
   "source": [
    "`pyenv` and `venv` are both popular tools in the Python community, but they serve slightly different purposes:\n",
    "\n",
    "1. **pyenv**: is used for managing multiple versions of Python on a single machine. You might use pyenv if, for example, you have some projects that require Python 3.7 and others that require Python 3.8. With pyenv, you can easily switch between these Python versions on a per-project basis.\n",
    "\n",
    "   Pyenv does not inherently isolate Python packages between environments; it's just about managing Python versions. However, an additional tool called `pyenv-virtualenv` can be used in conjunction with pyenv to manage both Python versions and dependencies in isolation.\n",
    "\n",
    "2. **venv**: is a module, included in Python 3.3 and later, used to create isolated Python environments. It lets you create an environment where you can install Python packages without affecting other environments or your system Python. This is useful when you have different projects that require different packages or versions of packages.\n",
    "\n",
    "So basically:\n",
    "\n",
    "- If you need to switch between multiple versions of Python, you might want to use pyenv.\n",
    "- If you need to isolate Python packages to avoid conflicts between different projects, you can use venv.\n",
    "- If you need to do both, you might consider using pyenv with the pyenv-virtualenv plugin.\n",
    "\n",
    "It's important to note that there are many tools available for managing Python environments, and the best one for you depends on your specific use case. Other popular options include virtualenv, pipenv, and conda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91502bc3-cfae-4527-babd-9ad60f48b1bd",
   "metadata": {},
   "source": [
    "# ðŸš— Back to WagonCab ðŸš— - read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45df656-7475-4185-a525-cc48fe5ea02c",
   "metadata": {},
   "source": [
    "# WagonCab has the huge, public"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be8baed-4267-45d5-aab2-36062103d560",
   "metadata": {},
   "source": [
    "# Your goal as an ML engineer will be to:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e70931-e5bc-4285-b299-0c6eb51742f6",
   "metadata": {},
   "source": [
    "# 3 Unit 1) Train at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a65608-afe4-4f3b-9609-5ea39e386b1c",
   "metadata": {},
   "source": [
    "# 1) Packaging & Virtual Env 101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f916574-1cb1-40f5-aaee-7fece91a2db6",
   "metadata": {},
   "source": [
    "In Python, a package is a way to organize and structure related modules (Python files) and sub-packages (nested packages) in a hierarchical manner. It allows you to group related functionality together, making it easier to manage and organize your code.\n",
    "\n",
    "A package is essentially a directory (or folder) that contains a special file called __init__.py. This file serves as an indicator that the directory should be treated as a package. The __init__.py file can be empty, or it can contain initialization code that is executed when the package is imported.\n",
    "\n",
    "By organizing your code into packages, you can create a modular and reusable structure for your Python projects. It helps avoid naming conflicts, provides a clear namespace, and allows for better organization and maintainability of code.\n",
    "\n",
    "To use a package in Python, you typically import the specific modules or sub-packages you need from the package using the import statement. \n",
    "\n",
    "And you can share it with others by choosing a distribution method:\n",
    "Some popular options include:\n",
    "\n",
    "- PyPI (Python Package Index): PyPI is the official repository for Python packages. You can upload your package to PyPI, making it easily accessible to other Python users who can install it using tools like pip. To publish your package on PyPI, you'll need to create a setup.py file that describes your package and its dependencies.\n",
    "\n",
    "- GitHub: Host your package on a version control platform like GitHub. This allows users to clone or download your package directly from the repository. You can also provide installation instructions and documentation in the repository's README file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65a8df2-2e71-4e60-9dfd-a5f2f2aaffcc",
   "metadata": {},
   "source": [
    "# Anatomy of a Minimal Python Package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75959acb-0ac0-440f-bd9e-b0cec1a879da",
   "metadata": {},
   "source": [
    "**running as python file x python module**\n",
    "\n",
    "The main difference between executing Python as a module and as a file is in how the code is used and accessed. Running Python as a module allows you to import and use the module's functionality within other code, providing a way to organize and reuse code. On the other hand, executing Python as a file runs the code directly from the file, making it suitable for standalone scripts or command-line tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a560ea7c-5b35-4ae1-8d1d-1e5eae3e28b4",
   "metadata": {},
   "source": [
    "# 1.2) Virtual Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036a9b3f-1cd1-4f90-a30c-aaaa22d91ff7",
   "metadata": {},
   "source": [
    "Virtual environments are isolated Python environments that allow you to manage and control the dependencies and packages used by your Python projects. They provide a way to create an isolated environment where you can install specific versions of Python packages without interfering with the system-wide Python installation or other projects.\n",
    "\n",
    "They are useful when you need to work on multiple projects with different package requirements or when you want to avoid conflicts between different versions of packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b4c3d7-7684-4ff9-9160-7cabd625a45e",
   "metadata": {},
   "source": [
    "# Reminder on pyenv vs. venv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054a36d0-c8b2-4885-866e-e39301dffd90",
   "metadata": {},
   "source": [
    "In a nutshell:\n",
    "    \n",
    "   Pyenv is primarily used for managing different versions of Python globally on your system, while venv is used to create isolated virtual environments specific to individual projects, allowing you to manage project-specific dependencies and packages. Both tools serve different purposes and can be used together in Python development workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e489190-ad18-4789-8565-5f7d25db302e",
   "metadata": {},
   "source": [
    "# 2) ðŸ’» Installing & Using a Package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b751f4f4-bbe0-4e9c-9dcb-64cfd44c11b5",
   "metadata": {},
   "source": [
    "# 2.1) Install the Package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829ba15b-3237-4578-979b-8074a7ee3bf4",
   "metadata": {},
   "source": [
    "The setup.py file is a script used in Python packages to define the metadata and dependencies of the package, as well as to specify the installation process. It is a crucial component when distributing and installing Python packages using the Python package manager, such as pip.\n",
    "\n",
    "The primary purpose of the setup.py file is to provide information about the package to the package management tools and to automate the installation process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21961c14-020e-4923-800b-3b9757325837",
   "metadata": {},
   "source": [
    "# 2.2) Run the Package from Anywhere (when totoenv is Activated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3621d7fc-5e99-4475-b698-6c5e2da14f4d",
   "metadata": {},
   "source": [
    "# For instance, from a notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061d8d6b-1a59-4af5-b187-5fbcb83bb178",
   "metadata": {},
   "source": [
    "# â“ Why Does it Work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fcc4d3-67a6-48fc-bd1e-020e4bdd202e",
   "metadata": {},
   "source": [
    "# site-packages contains all your pip packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86830bd0-03a1-43c6-bbbc-7676668ed389",
   "metadata": {},
   "source": [
    "The \"site-packages\" folder in Python is a location where third-party packages are installed by default. It is a directory where Python searches for and imports additional packages or modules that are not part of the standard library or the Python interpreter itself.\n",
    "\n",
    "When you install a third-party package using tools like pip, the package and its associated files (including modules, libraries, and other resources) are typically installed in the \"site-packages\" directory. This allows Python to locate and import those packages when they are needed in your code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd467fe6-23da-4545-bf42-59149e358200",
   "metadata": {},
   "source": [
    "# 2.3) Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2593ce54-6011-470d-b227-f324a6872f51",
   "metadata": {},
   "source": [
    "# 2.4) Adding a Makefile to Create Simple CLI Commands âš¡ï¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458957b3-920e-4dd1-8955-deb687a8bfbb",
   "metadata": {},
   "source": [
    "A Makefile is a special type of file used in software development projects to define and automate the build process. It contains a set of rules and instructions that specify how to compile source code, link dependencies, generate executables or libraries, and perform other build-related tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411a563e-7c70-49ea-b46c-262f75558576",
   "metadata": {},
   "source": [
    "# 3) Testing your Package ðŸ§ª"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6b2b95-845f-4110-903e-461b5ccf48e1",
   "metadata": {},
   "source": [
    "Testing plays a crucial role in software development, and there are several reasons why it is important to incorporate testing into the development process:\n",
    "- Identifying bugs and errors\n",
    "- Ensuring functionality\n",
    "- Maintaining code quality:\n",
    "- Enabling refactoring and code changes\n",
    "- Improving software maintainability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b08a643-057f-4814-bdfe-ffec515a398f",
   "metadata": {},
   "source": [
    "\n",
    "TDD stands for Test-Driven Development. It is a software development methodology that emphasizes writing tests before writing the actual code. The TDD approach follows a cycle of \"Red-Green-Refactor\" and involves the following steps:\n",
    "- Write a test\n",
    "- Run the test (Red)\n",
    "- Write the code (Green)\n",
    "- Run the test again\n",
    "- Refactor With the test passing, you can refactor the code to improve its design, structure, or performance without changing its behavior. The aim is to maintain or enhance the code's quality while keeping the tests passing\n",
    "- Repeat: The cycle repeats for each new functionality or change. you will continue writing new tests, implementing the necessary code to pass them, and then refactoring as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bea05f-e15e-40c1-aa35-85455e9ef95b",
   "metadata": {},
   "source": [
    "Let's code an example together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805cf218-c0b8-468f-9bd5-ed955a0ff3a6",
   "metadata": {},
   "source": [
    "1. create toto/divide.py\n",
    "2. vreate tests folder\n",
    "3. create test_divide.py\n",
    "4. paste code from slide - empy funtcion\n",
    "5. pip install pytest\n",
    "6. run pytest\n",
    "7. pytest -v tests/test_divide.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92241d29-9228-493c-bbdb-cbda18724717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create divide.py\n",
    "\n",
    "def divide_without_raising(x:float, y:float) -> float:\n",
    "    '''\n",
    "    divides x by y, but instead of raising errors when y equals 0, returns:\n",
    "    - inf if x positive\n",
    "    - -inf if x negative\n",
    "    - nan if x equals 0\n",
    "    '''\n",
    "    x/y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b0af44-af4f-4d4b-a700-2b18189938e0",
   "metadata": {},
   "source": [
    "but when x = 2.0, y = 0.0 it raise a xero divisio error - it breaks before the assertion. it breaks befora y == 0 so lets take care of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59261f52-6cc7-40ae-92c2-21e7b3d0de21",
   "metadata": {},
   "outputs": [],
   "source": [
    "if y != 0.:\n",
    "    return x/y\n",
    "else:\n",
    "    if x > 0.:\n",
    "        return float('inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f455ac52-1e2e-49ef-b319-242cf2e6fdce",
   "metadata": {},
   "source": [
    "But now it doent work for -1. lets fix that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a359286b-f675-417a-8028-038d6adb30cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "    if x < 0.:\n",
    "        return -1 * float('inf')\n",
    "    if x == 0.:\n",
    "        return float('nan')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65ca683-fbe0-46b4-b974-e3024e1457f1",
   "metadata": {},
   "source": [
    "Now our test passed. and thats the ideia of TDD. You get the idea... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8744ba46-9031-4783-8c2f-0dd1d6241440",
   "metadata": {},
   "source": [
    "now we can add make test to our make file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55db2e81-3950-459b-b65e-a9340f2d9bd6",
   "metadata": {},
   "source": [
    "Verbose output refers to additional detailed information displayed during the execution of a program or command. Verbose output provides more extensive and comprehensive feedback, often including additional details, debug information, or progress updates.Verbose output refers to additional detailed information displayed during the execution of a program or command. In the context of software development and testing, verbose output provides more extensive and comprehensive feedback, often including additional details, debug information, or progress updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcab643-33fe-42ce-9d92-66c9cac2e883",
   "metadata": {},
   "source": [
    "# Data engineering tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb211f9-f5dd-43be-8f51-814c2262f0a2",
   "metadata": {},
   "source": [
    "### Become a debugginf master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdac47b8-5582-415b-8501-f8adc0c8d87b",
   "metadata": {},
   "source": [
    "Debugging in data engineering often involves tracing errors in data pipelines, fixing data quality issues, and resolving performance bottlenecks. Here are some tips for debugging in the context of data engineering:\n",
    "\n",
    "1. **Understand Your Data:** Always know what your data looks like, its format, and how it should behave. Check data samples regularly to ensure consistency. Using data profiling or data quality tools can help in this understanding. \n",
    "\n",
    "2. **Thorough Logging:** Log as much information as you can during your data pipeline processes. Include details like timestamps, data size, data source, data destination, and any errors or warnings. These logs will be invaluable when diagnosing problems.\n",
    "\n",
    "3. **Use Monitoring Tools:** There are many tools available for monitoring data pipelines and jobs, such as Apache NiFi, AWS CloudWatch, Google Stackdriver, etc. These tools help in real-time debugging and alerting of any issues in your data pipelines.\n",
    "\n",
    "4. **Unit Testing:** Test each component of your pipeline individually to ensure it functions as expected. This will make debugging easier, as you can isolate issues to specific units.\n",
    "\n",
    "5. **Data Validation:** Validate your data at every stage of the pipeline. Check for nulls, incorrect formats, unexpected values, and other anomalies that could cause errors downstream.\n",
    "\n",
    "6. **Performance Metrics:** Monitor the performance metrics of your pipelines like run-time, resource usage, and throughput. This can help identify bottlenecks or performance degradation over time.\n",
    "\n",
    "7. **Replicate Issues Locally:** If you're debugging a complex issue, try to replicate it in a local or staging environment. This allows you to experiment and solve the problem without affecting the production environment.\n",
    "\n",
    "8. **Version Control:** Keep your pipeline scripts/code in version control systems like Git. This allows you to track changes over time, understand what changed when a bug appeared, and easily roll back changes if needed.\n",
    "\n",
    "9. **Incremental Testing:** When developing new features in your data pipeline, test incrementally. Don't wait until everything is built to begin testing. Test after adding each new feature or component.\n",
    "\n",
    "10. **Document Your Pipelines:** Document your data pipelines, including each component and how data flows through them. This documentation will be incredibly useful when you're trying to understand how to debug a problem.\n",
    "\n",
    "Remember, debugging is often about patience and systematic exploration. Take your time, be thorough, and you'll find the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec8a1b75-b887-40c4-ba81-baeffe5152dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide.py\n",
    "\n",
    "from toto.lib import who_am_i\n",
    "\n",
    "def divide_without_raising(x:float, y:float) -> float:\n",
    "    '''\n",
    "    divides x by y, but instead of raising errors when y equals 0, returns:\n",
    "    - inf if x positive\n",
    "    - -inf if x negative\n",
    "    - nan if x equals 0\n",
    "    '''\n",
    "    who_am_i()\n",
    "        \n",
    "    if y != 0.:\n",
    "        return x/y\n",
    "    else:\n",
    "        if x > 0.:\n",
    "            return float('inf')\n",
    "        if x < 0.:\n",
    "            return -1 * float('inf')\n",
    "        if x == 0.:\n",
    "            return float('nan')\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    divide_without_raising(2., 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f57e31a-88b7-49d2-bcde-c0d232986b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "python toto/divide.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec11ae6-5bda-4981-b0d8-0221d1fbf229",
   "metadata": {},
   "source": [
    "So this is a stack trace. , is a report showing the nested subroutine calls made by a program at a specific point in time. they are typically used for debugging purposes to track the sequence of nested functions called - in reverse order - up until the moment the stack trace is generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76122ff-b0cf-4b84-8f3e-74117a412489",
   "metadata": {},
   "source": [
    "Wwe can set trace manually as well, as a breakpoint for you code to pusposelly break at a certain point.\n",
    "1. pip install ipdb\n",
    "\n",
    "- add try except:\n",
    "1. The try block contains code that might potentially raise an exception. When an exception is raised, the rest of the code in the try block is skipped.\n",
    "\n",
    "2. After the try block, one or more except blocks catch and handle exceptions. Each except block specifies the type of exception it can handle. When an exception is raised in the try block, Python looks for an except block that handles this type of exception, and then executes that except block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee1b6b42-be1d-4b40-a42e-a37c5f8c4e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can't divide by zero!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # This code may raise an exception\n",
    "    x = 1 / 0\n",
    "except ZeroDivisionError:\n",
    "    # This code is executed if a ZeroDivisionError is raised\n",
    "    print(\"You can't divide by zero!\")\n",
    "    \n",
    "# In this example, the try block contains code that raises a ZeroDivisionError.\n",
    "# Since the try block can't complete successfully, Python raises an exception.\n",
    "# Then it finds the except block that handles ZeroDivisionError, and executes\n",
    "# the code in that block, printing the message \"You can't divide by zero!\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f60b39-25ef-4aa9-b469-fa32c00d504c",
   "metadata": {},
   "source": [
    "# Master your IDE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cdfdba-5b81-44fb-8990-94009c877304",
   "metadata": {},
   "source": [
    "# VS Code Shortcuts (macOS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5e39cb-4c53-48a5-bfa5-81a16a9d6626",
   "metadata": {},
   "source": [
    "# Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c083861b-f9e4-4a14-b964-f0535226fda6",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287c1265-494c-4fac-beed-58509a798e38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
